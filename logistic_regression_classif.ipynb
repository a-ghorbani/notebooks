{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"text-align:center;\"> Classification, scores and confusion distribution </h1>\n",
    "\n",
    "In order to easily remember how to do classification in Python (at least using `scikit-learn`) \n",
    "I put here some of the examples given in the webpage of [Scikit-learn](http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html).\n",
    "\n",
    "And a few examples for how to score the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "import seaborn as sns\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for classification\n",
    "\n",
    "`Scikit-learn` has a nice function to generate artificial data (namely `sklearn.datasets.make_classification`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nrows</th>\n",
       "      <th>ncols</th>\n",
       "      <th>nrows_C0</th>\n",
       "      <th>nrows_C1</th>\n",
       "      <th>C0 ratio</th>\n",
       "      <th>C1 ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>30000</td>\n",
       "      <td>10</td>\n",
       "      <td>20956</td>\n",
       "      <td>9044</td>\n",
       "      <td>0.6985333</td>\n",
       "      <td>0.3014667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>training</th>\n",
       "      <td>18000</td>\n",
       "      <td>10</td>\n",
       "      <td>12545</td>\n",
       "      <td>5455</td>\n",
       "      <td>0.6969444</td>\n",
       "      <td>0.3030556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>testing</th>\n",
       "      <td>12000</td>\n",
       "      <td>10</td>\n",
       "      <td>8411</td>\n",
       "      <td>3589</td>\n",
       "      <td>0.7009167</td>\n",
       "      <td>0.2990833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nrows ncols nrows_C0 nrows_C1   C0 ratio   C1 ratio\n",
       "data      30000    10    20956     9044  0.6985333  0.3014667\n",
       "training  18000    10    12545     5455  0.6969444  0.3030556\n",
       "testing   12000    10     8411     3589  0.7009167  0.2990833"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# generate skwed data (the ratio of class 1 to class 0 is defined by weights)\n",
    "X, y = datasets.make_classification(n_samples=30000, n_features=10,\n",
    "                                    n_informative=2, n_redundant=2,\n",
    "                                    weights = [0.7]\n",
    "                                    )\n",
    "# use 60% of the data for training\n",
    "train_ratio = 0.6\n",
    "train_samples = int(len(y) * train_ratio)\n",
    "\n",
    "X_train = X[:train_samples]\n",
    "X_test = X[train_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_test = y[train_samples:]\n",
    "\n",
    "# Create a DataFrame about few statistics of the data generated for only illustration purpose  \n",
    "counts = pd.DataFrame(columns=[\"nrows\", \"ncols\", \"nrows_C0\", \"nrows_C1\", \"C0 ratio\", \"C1 ratio\"],\n",
    "                      index=[\"data\", \"training\", \"testing\"])\n",
    "counts.loc[\"data\",] = [X.shape[0], X.shape[1], \n",
    "                       sum(y==0), sum(y==1), \n",
    "                       1.*sum(y==0)/len(y), 1.*sum(y==1)/len(y)]\n",
    "counts.loc[\"training\",] = [X_train.shape[0], X_train.shape[1], \n",
    "                           sum(y_train==0), sum(y_train==1), \n",
    "                           1.*sum(y_train==0)/len(y_train), 1.*sum(y_train==1)/len(y_train)]\n",
    "counts.loc[\"testing\",] = [X_test.shape[0], X_test.shape[1], \n",
    "                          sum(y_test==0), sum(y_test==1), \n",
    "                          1.*sum(y_test==0)/len(y_test), 1.*sum(y_test==1)/len(y_test)]\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification using scikit-learn\n",
    "\n",
    "Now lets start training the models.\n",
    "\n",
    "I have just taken the same models as it was in the example given [here](http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create classifiers\n",
    "lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "svc = LinearSVC(C=1.0)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# DataFrame for prediction probabilities \n",
    "probs_df = pd.DataFrame(columns=[\"Logistic\",\n",
    "                                \"Naive Bayes\",\n",
    "                                'Support Vector Classification',\n",
    "                                'Random Forest'])\n",
    "\n",
    "# DataFrame for score\n",
    "score_df = pd.DataFrame(columns=[\"Logistic\",\n",
    "                                \"Naive Bayes\",\n",
    "                                'Support Vector Classification',\n",
    "                                'Random Forest'],\n",
    "                      index=[\"Accuracy\", \"auc\", \"log score\", \"PPV\", \"TPR\", \"F1Measure\"])\n",
    "for clf, name in [(lr, 'Logistic'),\n",
    "                  (gnb, 'Naive Bayes'),\n",
    "                  (svc, 'Support Vector Classification'),\n",
    "                  (rfc, 'Random Forest')]:\n",
    "    # fit a model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # predict \n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    # obtaion probabilities\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        probs = clf.predict_proba(X_test)\n",
    "    else:  # use decision function\n",
    "        prob_pos = clf.decision_function(X_test)\n",
    "        prob_pos = \\\n",
    "            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "        probs = np.array([1-prob_pos,prob_pos]).T\n",
    "    \n",
    "    # probabilities for class 1\n",
    "    probs_df[name] = probs[:,1]\n",
    "    \n",
    "    # calculate few scores for the predictions\n",
    "    score_df[name] = pd.DataFrame([metrics.accuracy_score(y_test, predicted),\n",
    "                                  metrics.roc_auc_score(y_test, probs[:,1]),\n",
    "                                  1-metrics.log_loss(y_test, probs),\n",
    "                                  metrics.precision_score(y_test, predicted),\n",
    "                                  metrics.recall_score(y_test, predicted),\n",
    "                                  metrics.f1_score(y_test, predicted)], \n",
    "                                  index=[\"Accuracy\", \"auc\", \"log score\", \"PPV\", \"TPR\", \"F1Measure\"])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores - more illustrative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "ax = score_df.T.plot(kind=\"bar\", xlim=[-0.5,5.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feel the results of the models' prediction\n",
    "\n",
    "I like the following plots very much. The let you feel what really did your models.\n",
    "\n",
    "The plot are inspired by the equivalent (more or less -- I couldn't do better;) ) R code from [joyofdata](https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_pred_type_distribution.R)\n",
    "\n",
    "What is important in these plots to observe is to look at the plots and the distribution of the prediction and comparing it with the measure shown above.\n",
    "\n",
    "It is clear that at least for this case (e.g. when data is skewed regarding classes i.e. 70% 0s and 30% 1s) `accuracy` alone is not a good measure.\n",
    "\n",
    "If we rank the model based on their `accuracy`, SVM would be the second. But from the plot blow it is clear that it is not the case. It seems that `log-score` represent better the goodness of the classification based on what we can observe obtically in the plots shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_pred_type_distribution(prob_pos, true_value, threshold, title=\"\", ax=None):\n",
    "    \"\"\" Plot confusion scatter \n",
    "    \n",
    "        --  The code is inspired by :\n",
    "            The joyofdata: \n",
    "            https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_pred_type_distribution.R\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"prob_pos\", \"true_value\", \"pred_type\"])\n",
    "    df.prob_pos = prob_pos\n",
    "    df.true_value = true_value\n",
    "\n",
    "    df.loc[(df.prob_pos >= threshold) & (df.true_value == 1),\"pred_type\"] = \"TP\"\n",
    "    df.loc[(df.prob_pos >= threshold) & (df.true_value == 0),\"pred_type\"] = \"FP\" \n",
    "    df.loc[(df.prob_pos <  threshold) & (df.true_value == 1),\"pred_type\"] = \"FN\" \n",
    "    df.loc[(df.prob_pos <  threshold) & (df.true_value == 0),\"pred_type\"] = \"TN\"\n",
    "  \n",
    "    ax = sns.stripplot(x=\"prob_pos\", y=\"true_value\", data=df, size=5,\n",
    "                       orient=\"h\", hue=\"pred_type\", split= False, \n",
    "                       jitter=0.4, ax=ax, alpha=0.1)\n",
    "    ax = sns.violinplot(x=\"prob_pos\", y=\"true_value\", data=df, inner=None, \n",
    "                        color='#FFFFFF', orient=\"h\", scale=\"count\", ax=ax)\n",
    "    ax.set_ylim(2,-1)\n",
    "    ax.set_title(title, size=14)\n",
    "    ax.set_xlabel(\"prediction\", size=14)\n",
    "    ax.set_ylabel(\"True value\", size=14)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
    "fig.set_size_inches(15, 10)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "plot_pred_type_distribution(probs_df[\"Logistic\"], y_test, 0.5, \"Logistic\" , ax = ax1)\n",
    "plot_pred_type_distribution(probs_df[\"Naive Bayes\"], y_test, 0.5, \"Naive Bayes\", ax = ax2)\n",
    "plot_pred_type_distribution(probs_df[\"Support Vector Classification\"], y_test, 0.5, \"Support Vector Classification\", ax=ax3)\n",
    "plot_pred_type_distribution(probs_df[\"Random Forest\"], y_test, 0.5, \"Random Forest\", ax=ax4)\n",
    "txt = fig.suptitle(\"confusion scatter plot\", size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
