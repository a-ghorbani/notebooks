{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import containers\n",
    "from keras.layers.core import Dense, AutoEncoder\n",
    "from keras.activations import sigmoid\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Training the layer 1: Input 784 -> Output 600\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 31s - loss: 0.2386    \n",
      "Training the layer 2: Input 600 -> Output 500\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 29s - loss: 0.0222    \n",
      "Training the layer 3: Input 500 -> Output 400\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 27s - loss: 0.0273    \n",
      "Test score before fine turning: 2.53544282379\n",
      "Test accuracy after fine turning: 0.1135\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 172s - loss: 0.7888 - acc: 0.7341 - val_loss: 0.3558 - val_acc: 0.8968\n",
      "Test score after fine turning: 0.355791935372\n",
      "Test accuracy after fine turning: 0.8968\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 1\n",
    "nb_hidden_layers = [784, 600, 500, 400]\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# Layer-wise pretraining\n",
    "encoders = []\n",
    "nb_hidden_layers = [784, 600, 500, 400]\n",
    "X_train_tmp = np.copy(X_train)\n",
    "for i, (n_in, n_out) in enumerate(zip(nb_hidden_layers[:-1], nb_hidden_layers[1:]), start=1):\n",
    "    print('Training the layer {}: Input {} -> Output {}'.format(i, n_in, n_out))\n",
    "    # Create AE and training\n",
    "    ae = Sequential()\n",
    "    encoder = containers.Sequential([Dense(n_out, input_dim=n_in, activation='sigmoid')])\n",
    "    decoder = containers.Sequential([Dense(n_in, input_dim=n_out, activation='sigmoid')])\n",
    "    ae.add(AutoEncoder(encoder=encoder, decoder=decoder,\n",
    "                       output_reconstruction=False )) #, tie_weights=True))\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "    \n",
    "    ae.compile(loss='mean_squared_error', optimizer=optimizer) #'rmsprop')\n",
    "    ae.fit(X_train_tmp, X_train_tmp, batch_size=batch_size, nb_epoch=nb_epoch)\n",
    "    # Store trainined weight and update training data\n",
    "    encoders.append(ae.layers[0].encoder)\n",
    "    X_train_tmp = ae.predict(X_train_tmp)\n",
    "\n",
    "# Fine-turning\n",
    "model = Sequential()\n",
    "for encoder in encoders:\n",
    "    model.add(encoder)\n",
    "model.add(Dense(nb_classes, input_dim=nb_hidden_layers[-1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score before fine tuning:', score[0])\n",
    "print('Test accuracy before fine tuning:', score[1])\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          show_accuracy=True, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score after fine tuning:', score[0])\n",
    "print('Test accuracy after fine tuning:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Training the layer 1: Input 784 -> Output 600\n",
      "Training the layer 2: Input 600 -> Output 500\n",
      "Training the layer 3: Input 500 -> Output 400\n",
      "Test score before fine tuning: 2.42104121437\n",
      "Test accuracy before fine tuning: 0.0974\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 200s - loss: 0.7848 - acc: 0.7383 - val_loss: 0.3545 - val_acc: 0.8931\n",
      "Test score after fine tuning: 0.354463977933\n",
      "Test accuracy after fine tuning: 0.8931\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 1\n",
    "nb_hidden_layers = [784, 600, 500, 400]\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# Layer-wise pretraining\n",
    "encoders = []\n",
    "nb_hidden_layers = [784, 600, 500, 400]\n",
    "X_train_tmp = np.copy(X_train)\n",
    "for i, (n_in, n_out) in enumerate(zip(nb_hidden_layers[:-1], nb_hidden_layers[1:]), start=1):\n",
    "    print('Training the layer {}: Input {} -> Output {}'.format(i, n_in, n_out))\n",
    "    # \n",
    "    encoder = containers.Sequential([Dense(n_out, input_dim=n_in, activation='sigmoid')])\n",
    "    encoders.append(encoder)\n",
    "    \n",
    "# Fine-turning\n",
    "model = Sequential()\n",
    "for encoder in encoders:\n",
    "    model.add(encoder)\n",
    "model.add(Dense(nb_classes, input_dim=nb_hidden_layers[-1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score before fine tuning:', score[0])\n",
    "print('Test accuracy before fine tuning:', score[1])\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          show_accuracy=True, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score after fine tuning:', score[0])\n",
    "print('Test accuracy after fine tuning:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 600, 500]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_hidden_layers[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_hidden_layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
